{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ALCNN_model",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYyhyeDKYrAV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import keras\n",
        "  \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from cv2 import imread, createCLAHE \n",
        "import cv2\n",
        "from glob import glob\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import keras.backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras import Model\n",
        "from keras.callbacks import  ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
        "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
        "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "from tqdm import tqdm_notebook\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.engine import InputSpec\n",
        "from keras import backend as K\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.callbacks as callbacks\n",
        "from keras.callbacks import Callback\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.layers import multiply\n",
        "\n",
        "from sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, ReLU,LSTM, Bidirectional\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import models, layers, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.layers.core import Activation, SpatialDropout2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "    \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"drive/My Drive/trainpneumo/\"\n",
        "test_dir =  \"drive/My Drive/testpneumo/\"\n",
        "#valid_dir = \"drive/My Drive/validK10/\"\n",
        "def get_data(folder):\n",
        "    X = []\n",
        "    y = []\n",
        "    for folderName in os.listdir(folder):\n",
        "        if not folderName.startswith('.'):\n",
        "            if folderName in ['normal']:\n",
        "                label = 0\n",
        "            elif folderName in ['pneumothorax']:\n",
        "                label = 1\n",
        "          #  elif folderName in ['pneumonia']:\n",
        "           #     label = 2  \n",
        "            else:\n",
        "                label = 2\n",
        "            for image_filename in tqdm(os.listdir(folder + folderName)):\n",
        "                img_file = cv2.imread(folder + folderName + '/' + image_filename)\n",
        "                if img_file is not None:\n",
        "                    #img_file = skimage.transform.resize(img_file, (384,384,3))\n",
        "                    #img_file = scipy.misc.imresize(arr=img_file, size=(150, 150, 3))\n",
        "                    img_arr = np.asarray(img_file)\n",
        "                    X.append(img_arr)\n",
        "                    y.append(label)\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    return X,y\n",
        "X_data, y_data = get_data(data_dir)\n",
        "X_test, y_test= get_data(test_dir)\n",
        "X_valid, y_valid = get_data(valid_dir)\n",
        "Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_dataHot = to_categorical(y_data, num_classes = 2)\n",
        "y_testHot = to_categorical(y_test, num_classes = 2)\n",
        "y_validHot = to_categorical(y_valid, num_classes=2)"
      ],
      "metadata": {
        "id": "-gZWaeGFQk5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhg4M1yqaqIX"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP84bbzDasiu"
      },
      "source": [
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "class MetricsCheckpoint(Callback):\n",
        "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
        "    def __init__(self, savepath):\n",
        "        super(MetricsCheckpoint, self).__init__()\n",
        "        self.savepath = savepath\n",
        "        self.history = {}\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        np.save(self.savepath, self.history)\n",
        "\n",
        "def plotKerasLearningCurve():\n",
        "    plt.figure(figsize=(10,5))\n",
        "    metrics = np.load('logs.npy')[()]\n",
        "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
        "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
        "        l = np.array(metrics[k])\n",
        "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
        "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
        "        y = l[x]\n",
        "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
        "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
        "    plt.legend(loc=4)\n",
        "    plt.axis([0, None, None, None]);\n",
        "    plt.grid()\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "def plot_learning_curve(history):\n",
        "    plt.figure(figsize=(12,4),dpi=100)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('./accuracy_curve.png')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('./loss_curve.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFBYghwrauqf"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDo73veeawSI"
      },
      "source": [
        "from itertools import cycle\n",
        "def plot_roc(y_true,y_pred):      \n",
        "    #plt.style.use('ggplot')\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes=2\n",
        "    i=1\n",
        "    for i in range(n_classes):\n",
        "\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true, y_pred)\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    colors = cycle(['blue', 'red', 'green'])\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "      plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0},(area = {1:0.2f})' ''.format(i, roc_auc[i]))\n",
        "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    i+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObTdTp1wNGR6"
      },
      "source": [
        "def squeeze_excite_block(input, ratio=16):\n",
        "    ''' Create a squeeze-excite block\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "        k: width factor\n",
        "    Returns: a keras tensor\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        se = Permute((3, 1, 2))(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKziSglMa2x2"
      },
      "source": [
        "ROW_AXIS = 1\n",
        "COL_AXIS = 2\n",
        "CHANNEL_AXIS = 3\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "map_characters1 = {0: 'normal', 1: 'pneumothorax'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R67xAZ0yGQv-"
      },
      "source": [
        "\n",
        "def conv_1_architecture(X_train,y_train,X_valid,y_valid,X_test,y_test,numclasses,numepochs,labels):\n",
        "    input = Input((256,256,3))\n",
        "    x = Conv2D(16, kernel_size=(3,3),padding='same')(input)\n",
        "    x = BatchNormalization()\n",
        "    x = ReLU()(x)\n",
        "    se1 = squeeze_excite_block(x)\n",
        "    x = Conv2D(16, kernel_size=(3,3),padding='same')(se1)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    #x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "    x = Conv2D(32, kernel_size=(3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se2 = squeeze_excite_block\n",
        "    x = Conv2D(32, kernel_size=(3,3), padding='same')(se2)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    #x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "    x = Conv2D(64, kernel_size=(3,3),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se3 = squeeze_excite_block(x)\n",
        "    x = Conv2D(64, kernel_size=(3,3),padding='same')(se3)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    #x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "    x = Conv2D(128, kernel_size=(3,3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se4 = squeeze_excite_block(x)\n",
        "    x = Conv2D(128, kernel_size=(3,3), padding='same')(se4)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se5 = squeeze_excite_block(x)\n",
        "    x = Conv2D(128, kernel_size=(3,3), padding='same')(se5)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    #x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
        "    x = Conv2D(256, kernel_size=(3,3) ,padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se6 = squeeze_excite_block(x)\n",
        "    x = Conv2D(256, kernel_size=(3,3) ,padding='same')(se6)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    se7 = squeeze_excite_block(x)\n",
        "    x = Conv2D(256, kernel_size=(3,3) ,padding='same')(se7)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    #x = Reshape((49, 256))(x)\n",
        "    #x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=True, trainable=False))(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x= Dense(256, activation='relu')(x)\n",
        "    x= BatchNormalization()(x)\n",
        "    x = Dropout(0.20)(x)\n",
        "    x= Dense(128, activation='relu')(x)\n",
        "    x= BatchNormalization()(x)\n",
        "    x = Dropout(0.20)(x)\n",
        "    predictions = Dense(2, activation='softmax')(x)\n",
        "    model = Model(inputs=input, outputs=predictions)\n",
        "    model.summary()\n",
        "    model.compile(loss='binary_crossentropy', \n",
        "                  optimizer=Adam(learning_rate=0.001), \n",
        "                  metrics=['accuracy'])\n",
        "    weight_path=\"{}_weights.best.hdf5\".format('cxr_reg')\n",
        "\n",
        "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = True)\n",
        "\n",
        "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
        "                                   patience=4, \n",
        "                                   verbose=1, mode='min', min_delta=0.0001, cooldown=2, min_lr=1e-6)\n",
        "    early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=14) # probably needs to be more patient, but kaggle time is limited\n",
        "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
        "\n",
        "    #train_data, test_data, train_vol, test_vol = train_test_split(X_Data,y_DataHot, test_size = 0.2, shuffle=True)\n",
        "\n",
        "    # Fit model\n",
        "    history = model.fit(datagen.flow(X_train,y_train, batch_size=16), epochs=numepochs, validation_data=(X_valid,y_valid),callbacks=callbacks_list)\n",
        "    # Evaluate model\n",
        "    score = model.evaluate(X_test,y_test, verbose=0)\n",
        "    print('Model Accuracy:', score[1], '\\n')\n",
        "    print('Model Loss:', score[0])\n",
        "    y_validscore= model.predict(X_valid)\n",
        "    print('\\n', sklearn.metrics.classification_report(np.where(y_valid > 0)[1], np.argmax(y_validscore, axis=1), target_names=list(labels.values())), sep='') \n",
        "    y_pred = model.predict(X_test)\n",
        "    print('\\n', sklearn.metrics.classification_report(np.where(y_test > 0)[1], np.argmax(y_pred, axis=1), target_names=list(labels.values())), sep='') \n",
        "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
        "    Y_true = np.argmax(y_test,axis = 1) \n",
        "    \n",
        "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "    plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))\n",
        "    plt.show()\n",
        "    plot_learning_curve(history)\n",
        "    plt.show()\n",
        "    plot_roc(Y_true, Y_pred_classes)\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc \n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for train, valid in kfold.split(X_data, y_dataHot):\n",
        "  model = conv_1_architecture(X_data[train], y_dataHot[train], X_data[valid], y_dataHot[valid],X_test,y_testHot,2,65,map_characters1)\n",
        "  del model\n",
        "  tf.keras.backend.clear_session()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "THH5oLLCRMmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}